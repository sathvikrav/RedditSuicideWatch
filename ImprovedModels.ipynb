{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZPOfTbRN3vr"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets.load import load_metric\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lA2LRfxeaRDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertPreTrainedModel, PretrainedConfig, DistilBertModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from typing import Optional, Tuple, Union, Dict, List, Any"
      ],
      "metadata": {
        "id": "DJ7yeWYZnGOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our improved model, we are going to combine a sentiment feature with the textual features that our baseline BERT model has learned from Reddit post text. We will achieve this by performing feature concatenation in the pre_classifier layer (i.e. the layer before the classifier layer). We will also modify the weights of our classifier layer such that our new sentiment feature is assigned a weight of 1, and thus, has a larger impact on the final classification result."
      ],
      "metadata": {
        "id": "AjboDYqs25Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDistilBertForSequenceClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config: PretrainedConfig):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim + 1, config.num_labels)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_position_embeddings(self) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Returns the position embeddings\n",
        "        \"\"\"\n",
        "        return self.distilbert.get_position_embeddings()\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "        \"\"\"\n",
        "        Resizes position embeddings of the model if `new_num_position_embeddings != config.max_position_embeddings`.\n",
        "        Arguments:\n",
        "            new_num_position_embeddings (`int`):\n",
        "                The number of new position embedding matrix. If position embeddings are learned, increasing the size\n",
        "                will add newly initialized vectors at the end, whereas reducing the size will remove vectors from the\n",
        "                end. If position embeddings are not learned (*e.g.* sinusoidal position embeddings), increasing the\n",
        "                size will add correct vectors at the end following the position encoding algorithm, whereas reducing\n",
        "                the size will remove vectors from the end.\n",
        "        \"\"\"\n",
        "        self.distilbert.resize_position_embeddings(new_num_position_embeddings)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        sentiment: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        severity: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        distilbert_output = self.distilbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
        "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
        "\n",
        "        # Here is the magic! \n",
        "\n",
        "        pooled_output = torch.cat((pooled_output, sentiment.view(torch.numel(sentiment), -1)), dim=1)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        \n",
        "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
        "\n",
        "        loss = None\n",
        "        if severity is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (severity.dtype == torch.long or severity.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), severity.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, severity)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), severity.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, severity)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + distilbert_output[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=distilbert_output.hidden_states,\n",
        "            attentions=distilbert_output.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "i8y1tkwtm2JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, I have provided an example of how the model can be trained. It is important to remember that the severity label is a weak signal that we are using to train on each Reddit post, since labels should only be assigned to users rather than posts. Therefore, we employ the same strategy as we did for the baseline, where we use DistilBERT to train our Reddit posts on severity and then aggregate the Reddit posts and their classifications by users before using a one-level Decision Tree classifier to assign a label to each user."
      ],
      "metadata": {
        "id": "9Mz2FC0Sazv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ],
      "metadata": {
        "id": "vJ5tfzG6RGBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train_datapt = tokenizer.encode_plus(\"I want to take you away.\", padding='max_length', truncation=True)\n",
        "sample_train_datapt2 = tokenizer.encode_plus(\"I want to eat.\", padding='max_length', truncation=True)\n",
        "sample_test_datapt = tokenizer.encode_plus(\"I want to take you away.\", padding='max_length', truncation=True)\n",
        "dummytrain_df = pd.DataFrame({'input_ids': [sample_train_datapt['input_ids'], sample_train_datapt2['input_ids']], 'attention_mask': [sample_train_datapt['attention_mask'], sample_train_datapt2['attention_mask']], 'severity': [0, 1], 'sentiment': [1, 0]})\n",
        "dummytest_df = pd.DataFrame({'input_ids': [sample_test_datapt['input_ids']], 'attention_mask': [sample_test_datapt['attention_mask']], 'severity': [0], 'sentiment': [1]})"
      ],
      "metadata": {
        "id": "yOj__aJDRKnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummytrain_dataset = Dataset.from_pandas(dummytrain_df)\n",
        "dummytest_dataset = Dataset.from_pandas(dummytest_df)"
      ],
      "metadata": {
        "id": "vlmWciDpRe67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "          output_dir= \"distilbert-base-cased-checkpoint\",\n",
        "          do_train=True,\n",
        "          do_eval=True,\n",
        "          num_train_epochs=1,\n",
        "          evaluation_strategy='epoch',\n",
        "          label_names=['severity']\n",
        "        )"
      ],
      "metadata": {
        "id": "MG7qs_vPRg37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_categories = 2\n",
        "model = CustomDistilBertForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"distilbert-base-cased\", num_labels=num_categories)"
      ],
      "metadata": {
        "id": "ENUZAS3DRlqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "          model=model,\n",
        "          args=args,\n",
        "          train_dataset=dummytrain_dataset,\n",
        "          eval_dataset=dummytest_dataset,\n",
        "          tokenizer=tokenizer,\n",
        "        )"
      ],
      "metadata": {
        "id": "dYs17wwVRua_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "NIJiI5WARxCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to experiment with using multiple outputs. For example, we may want to use the sentiment label for each Reddit post as a weak signal to fine-tune our modified DistilBERT model on the task of high-severity vs. low-severity classification. For this, we introduce a second classifier layer for our sentiment label. Then, we modify our model training process by subclassing the Trainer API from the HuggingFace library."
      ],
      "metadata": {
        "id": "zI-OjgD2OLd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDistilBertMultipleOutputsForSequenceClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config: PretrainedConfig, sentiment_labels: int):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.num_sentiments = sentiment_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
        "\n",
        "        # Here, we increase the size of our classifier layer to accommodate for the extra feature(s)\n",
        "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
        "        self.sentiment_classifier = nn.Linear(config.dim, sentiment_labels)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_position_embeddings(self) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Returns the position embeddings\n",
        "        \"\"\"\n",
        "        return self.distilbert.get_position_embeddings()\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "        \"\"\"\n",
        "        Resizes position embeddings of the model if `new_num_position_embeddings != config.max_position_embeddings`.\n",
        "        Arguments:\n",
        "            new_num_position_embeddings (`int`):\n",
        "                The number of new position embedding matrix. If position embeddings are learned, increasing the size\n",
        "                will add newly initialized vectors at the end, whereas reducing the size will remove vectors from the\n",
        "                end. If position embeddings are not learned (*e.g.* sinusoidal position embeddings), increasing the\n",
        "                size will add correct vectors at the end following the position encoding algorithm, whereas reducing\n",
        "                the size will remove vectors from the end.\n",
        "        \"\"\"\n",
        "        self.distilbert.resize_position_embeddings(new_num_position_embeddings)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        sentiment_label: Optional[torch.Tensor] = None,\n",
        "        severity: Optional[torch.LongTensor] = None,\n",
        "        sentiment: Optional[torch.LongTensor] = None\n",
        "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        distilbert_output = self.distilbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
        "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "\n",
        "        logits_severity = self.classifier(pooled_output)  # (bs, num_labels)\n",
        "        logits_sentiment = self.sentiment_classifier(pooled_output)\n",
        "\n",
        "        loss_severity = None\n",
        "        loss_sentiment = None\n",
        "\n",
        "        if severity is not None:\n",
        "          loss_fct = CrossEntropyLoss()\n",
        "          loss_severity = loss_fct(logits_severity.view(-1, self.num_labels), severity)\n",
        "        \n",
        "        if sentiment is not None:\n",
        "          loss_fct = BCEWithLogitsLoss()\n",
        "          sentiment_labels = torch.FloatTensor([[1 if x == sentiment[i] else 0 for x in range(self.num_sentiments)] for i in range(torch.numel(sentiment))])\n",
        "          loss_sentiment = loss_fct(logits_sentiment.view(-1, self.num_sentiments), sentiment_labels)\n",
        "\n",
        "        return (loss_severity + loss_sentiment, SequenceClassifierOutput(\n",
        "            loss=loss_severity,\n",
        "            logits=logits_severity,\n",
        "            hidden_states=distilbert_output.hidden_states,\n",
        "            attentions=distilbert_output.attentions,\n",
        "        ), SequenceClassifierOutput(\n",
        "            loss=loss_sentiment,\n",
        "            logits=logits_sentiment,\n",
        "            hidden_states=distilbert_output.hidden_states,\n",
        "            attentions=distilbert_output.attentions,\n",
        "        ))"
      ],
      "metadata": {
        "id": "3O71IwzFOvaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create our custom trainer by subclassing the Trainer API. Because we need custom behavior for training our model with multiple outputs, we override the evaluation_loop and prediction_step methods."
      ],
      "metadata": {
        "id": "EmdxXLeOPEM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_pt_utils import IterableDatasetShard, nested_detach, nested_concat, nested_numpify, nested_truncate, find_batch_size\n",
        "from transformers.utils import is_sagemaker_mp_enabled\n",
        "from transformers.trainer_utils import EvalLoopOutput, EvalPrediction, denumpify_detensorize, has_length\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.deepspeed import deepspeed_init"
      ],
      "metadata": {
        "id": "K2-2Mb9n7aDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smp_forward_only(model, inputs):\n",
        "  return model(**inputs)\n",
        "\n",
        "def smp_nested_concat(tensor):\n",
        "  if isinstance(tensor, (list, tuple)):\n",
        "      return type(tensor)(smp_nested_concat(t) for t in tensor)\n",
        "  elif isinstance(tensor, dict):\n",
        "      return type(tensor)({k: smp_nested_concat(v) for k, v in tensor.items()})\n",
        "  # It doesn't seem possible to check here if `tensor` is a StepOutput because StepOutput lives in `smp.step`\n",
        "  # which is also the name of the decorator so Python is confused.\n",
        "  return tensor.concat().detach().cpu()"
      ],
      "metadata": {
        "id": "E3IA_KtuBCVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        return (outputs[0], outputs[1], outputs[2]) if return_outputs else outputs[0]\n",
        "    def evaluation_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str,\n",
        "        prediction_loss_only: Optional[bool] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "    ) -> EvalLoopOutput:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        args = self.args\n",
        "\n",
        "        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
        "\n",
        "        # if eval is called w/o train init deepspeed here\n",
        "        if args.deepspeed and not self.deepspeed:\n",
        "            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
        "            # from the checkpoint eventually\n",
        "            deepspeed_engine, _, _ = deepspeed_init(\n",
        "                self, num_training_steps=0, resume_from_checkpoint=None, inference=True\n",
        "            )\n",
        "            self.model = deepspeed_engine.module\n",
        "            self.model_wrapped = deepspeed_engine\n",
        "            self.deepspeed = deepspeed_engine\n",
        "\n",
        "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
        "\n",
        "        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n",
        "        # while ``train`` is running, cast it to the right dtype first and then put on device\n",
        "        if not self.is_in_train:\n",
        "            if args.fp16_full_eval:\n",
        "                model = model.to(dtype=torch.float16, device=args.device)\n",
        "            elif args.bf16_full_eval:\n",
        "                model = model.to(dtype=torch.bfloat16, device=args.device)\n",
        "\n",
        "        batch_size = self.args.eval_batch_size\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        self.callback_handler.eval_dataloader = dataloader\n",
        "        # Do this before wrapping.\n",
        "        eval_dataset = getattr(dataloader, \"dataset\", None)\n",
        "\n",
        "        if args.past_index >= 0:\n",
        "            self._past = None\n",
        "\n",
        "        # Initialize containers\n",
        "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
        "        losses_host = None\n",
        "        preds_host = None\n",
        "        labels_host = None\n",
        "        inputs_host = None\n",
        "\n",
        "        # losses/preds/labels on CPU (final containers)\n",
        "        all_losses = None\n",
        "        all_preds = None\n",
        "        all_labels = None\n",
        "        all_inputs = None\n",
        "        # Will be useful when we have an iterable dataset so don't know its length.\n",
        "\n",
        "        observed_num_examples = 0\n",
        "        # Main evaluation loop\n",
        "        for step, inputs in enumerate(dataloader):\n",
        "            # Update the observed num examples\n",
        "            observed_batch_size = find_batch_size(inputs)\n",
        "            if observed_batch_size is not None:\n",
        "                observed_num_examples += observed_batch_size\n",
        "                # For batch samplers, batch_size is not known by the dataloader in advance.\n",
        "                if batch_size is None:\n",
        "                    batch_size = observed_batch_size\n",
        "\n",
        "            # Prediction step\n",
        "            loss, logits_severity, logits_sentiment, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
        "            # print(self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys))\n",
        "            inputs_decode = self._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n",
        "\n",
        "            # Update containers on host\n",
        "            if loss is not None:\n",
        "                losses = self._nested_gather(loss.repeat(batch_size))\n",
        "                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
        "            if labels is not None:\n",
        "                labels = self._pad_across_processes(labels)\n",
        "                labels = self._nested_gather(labels)\n",
        "                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
        "            if inputs_decode is not None:\n",
        "                inputs_decode = self._pad_across_processes(inputs_decode)\n",
        "                inputs_decode = self._nested_gather(inputs_decode)\n",
        "                inputs_host = (\n",
        "                    inputs_decode\n",
        "                    if inputs_host is None\n",
        "                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n",
        "                )\n",
        "            \n",
        "            if logits_severity is not None:\n",
        "                logits_severity = self._pad_across_processes(logits_severity)\n",
        "                logits_severity = self._nested_gather(logits_severity)\n",
        "                if self.preprocess_logits_for_metrics is not None:\n",
        "                    logits = self.preprocess_logits_for_metrics(logits_severity, labels)\n",
        "                preds_host = logits_severity if preds_host is None else nested_concat(preds_host, logits_severity, padding_index=-100)\n",
        "            \n",
        "            if logits_sentiment is not None:\n",
        "                logits_sentiment = self._pad_across_processes(logits_sentiment)\n",
        "                logits_sentiment = self._nested_gather(logits_sentiment)\n",
        "                if self.preprocess_logits_for_metrics is not None:\n",
        "                    logits = self.preprocess_logits_for_metrics(logits_sentiment, labels)\n",
        "                preds_host = logits_sentiment if preds_host is None else nested_concat(preds_host, logits_sentiment, padding_index=-100)\n",
        "\n",
        "\n",
        "            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
        "\n",
        "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
        "            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
        "                if losses_host is not None:\n",
        "                    losses = nested_numpify(losses_host)\n",
        "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "                if preds_host is not None:\n",
        "                    logits = nested_numpify(preds_host)\n",
        "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "                if inputs_host is not None:\n",
        "                    inputs_decode = nested_numpify(inputs_host)\n",
        "                    all_inputs = (\n",
        "                        inputs_decode\n",
        "                        if all_inputs is None\n",
        "                        else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n",
        "                    )\n",
        "                if labels_host is not None:\n",
        "                    labels = nested_numpify(labels_host)\n",
        "                    all_labels = (\n",
        "                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "                    )\n",
        "\n",
        "                # Set back to None to begin a new accumulation\n",
        "                losses_host, preds_host, inputs_host, labels_host = None, None, None, None\n",
        "\n",
        "        if args.past_index and hasattr(self, \"_past\"):\n",
        "            # Clean the state at the end of the evaluation loop\n",
        "            delattr(self, \"_past\")\n",
        "\n",
        "        # Gather all remaining tensors and put them back on the CPU\n",
        "        if losses_host is not None:\n",
        "            losses = nested_numpify(losses_host)\n",
        "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
        "        if preds_host is not None:\n",
        "            logits = nested_numpify(preds_host)\n",
        "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
        "        if inputs_host is not None:\n",
        "            inputs_decode = nested_numpify(inputs_host)\n",
        "            all_inputs = (\n",
        "                inputs_decode if all_inputs is None else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n",
        "            )\n",
        "        if labels_host is not None:\n",
        "            labels = nested_numpify(labels_host)\n",
        "            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
        "\n",
        "        # Number of samples\n",
        "        if has_length(eval_dataset):\n",
        "            num_samples = len(eval_dataset)\n",
        "        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n",
        "        # methods. Therefore we need to make sure it also has the attribute.\n",
        "        elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
        "            num_samples = eval_dataset.num_examples\n",
        "        else:\n",
        "            if has_length(dataloader):\n",
        "                num_samples = self.num_examples(dataloader)\n",
        "            else:  # both len(dataloader.dataset) and len(dataloader) fail\n",
        "                num_samples = observed_num_examples\n",
        "        if num_samples == 0 and observed_num_examples > 0:\n",
        "            num_samples = observed_num_examples\n",
        "\n",
        "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
        "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
        "        if all_losses is not None:\n",
        "            all_losses = all_losses[:num_samples]\n",
        "        if all_preds is not None:\n",
        "            all_preds = nested_truncate(all_preds, num_samples)\n",
        "        if all_labels is not None:\n",
        "            all_labels = nested_truncate(all_labels, num_samples)\n",
        "        if all_inputs is not None:\n",
        "            all_inputs = nested_truncate(all_inputs, num_samples)\n",
        "\n",
        "        # Metrics!\n",
        "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
        "            if args.include_inputs_for_metrics:\n",
        "                metrics = self.compute_metrics(\n",
        "                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n",
        "                )\n",
        "            else:\n",
        "                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
        "        else:\n",
        "            metrics = {}\n",
        "\n",
        "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
        "        metrics = denumpify_detensorize(metrics)\n",
        "\n",
        "        if all_losses is not None:\n",
        "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
        "        if hasattr(self, \"jit_compilation_time\"):\n",
        "            metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n",
        "\n",
        "        # Prefix all keys with metric_key_prefix + '_'\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
        "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n",
        "    \n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Perform an evaluation step on `model` using `inputs`.\n",
        "\n",
        "        Subclass and override to inject custom behavior.\n",
        "\n",
        "        Args:\n",
        "            model (`nn.Module`):\n",
        "                The model to evaluate.\n",
        "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "                The inputs and targets of the model.\n",
        "\n",
        "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
        "            prediction_loss_only (`bool`):\n",
        "                Whether or not to return the loss only.\n",
        "            ignore_keys (`Lst[str]`, *optional*):\n",
        "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "                gathering predictions.\n",
        "\n",
        "        Return:\n",
        "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
        "            logits and labels (each being optional).\n",
        "        \"\"\"\n",
        "        logits_severity = None\n",
        "        logits_sentiment = None\n",
        "\n",
        "        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n",
        "        # For CLIP-like models capable of returning loss values.\n",
        "        # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n",
        "        # is `True` in `model.forward`.\n",
        "        return_loss = inputs.get(\"return_loss\", None)\n",
        "        if return_loss is None:\n",
        "            return_loss = self.can_return_loss\n",
        "        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n",
        "\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        if ignore_keys is None:\n",
        "            if hasattr(self.model, \"config\"):\n",
        "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
        "            else:\n",
        "                ignore_keys = []\n",
        "\n",
        "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
        "        if has_labels or loss_without_labels:\n",
        "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
        "            if len(labels) == 1:\n",
        "                labels = labels[0]\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if is_sagemaker_mp_enabled():\n",
        "                raw_outputs = smp_forward_only(model, inputs)\n",
        "                if has_labels or loss_without_labels:\n",
        "                    if isinstance(raw_outputs, dict):\n",
        "                        loss_mb = raw_outputs[\"loss\"]\n",
        "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n",
        "                    else:\n",
        "                        loss_mb = raw_outputs[0]\n",
        "                        logits_mb = raw_outputs[1:]\n",
        "\n",
        "                    loss = loss_mb.reduce_mean().detach().cpu()\n",
        "                    logits = smp_nested_concat(logits_mb)\n",
        "                else:\n",
        "                    loss = None\n",
        "                    if isinstance(raw_outputs, dict):\n",
        "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n",
        "                    else:\n",
        "                        logits_mb = raw_outputs\n",
        "                    logits = smp_nested_concat(logits_mb)\n",
        "            else:\n",
        "                if has_labels or loss_without_labels:\n",
        "                    with self.compute_loss_context_manager():\n",
        "                        loss, output_severity, output_sentiment = self.compute_loss(model, inputs, return_outputs=True)\n",
        "                    loss = loss.mean().detach()\n",
        "\n",
        "                    if isinstance(output_severity, dict):\n",
        "                        logits_severity = tuple(v for k, v in output_severity.items() if k not in ignore_keys + [\"loss\"])\n",
        "                    else:\n",
        "                        logits_severity = outputs[1:]\n",
        "                    \n",
        "                    if isinstance(output_sentiment, dict):\n",
        "                        logits_sentiment = tuple(v for k, v in output_sentiment.items() if k not in ignore_keys + [\"loss\"])\n",
        "                    else:\n",
        "                        logits_sentiment = outputs[1:]\n",
        "                else:\n",
        "                    loss = None\n",
        "                    with self.compute_loss_context_manager():\n",
        "                        outputs = model(**inputs)\n",
        "                    if isinstance(outputs, dict):\n",
        "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
        "                    else:\n",
        "                        logits = outputs\n",
        "                    # TODO: this needs to be fixed and made cleaner later.\n",
        "                    if self.args.past_index >= 0:\n",
        "                        self._past = outputs[self.args.past_index - 1]\n",
        "\n",
        "        if prediction_loss_only:\n",
        "            return (loss, None, None, None)\n",
        "\n",
        "        logits_severity = nested_detach(logits_severity)\n",
        "        if len(logits_severity) == 1:\n",
        "            logits_severity = logits_severity[0]\n",
        "        \n",
        "        logits_sentiment = nested_detach(logits_sentiment)\n",
        "        if len(logits_sentiment) == 1:\n",
        "            logits_sentiment = logits_sentiment[0]\n",
        "\n",
        "        return (loss, logits_severity, logits_sentiment, labels)"
      ],
      "metadata": {
        "id": "YU5iEQi5Xpz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, I have provided an example for how the model will be used. Similar to the previous custom DistilBERT model, we employ the same strategy as we did for the baseline, where we use DistilBERT to train our Reddit posts on severity AND sentiment and then aggregate the Reddit posts and their classifications by users before using a one-level Decision Tree classifier to assign a label to each user."
      ],
      "metadata": {
        "id": "evZypdbWbksY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ],
      "metadata": {
        "id": "dh0l4JGoRLWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train_datapt = tokenizer.encode_plus(\"I want to take you away.\", padding='max_length', truncation=True)\n",
        "sample_train_datapt2 = tokenizer.encode_plus(\"I want to eat.\", padding='max_length', truncation=True)\n",
        "sample_test_datapt = tokenizer.encode_plus(\"I want to take you away.\", padding='max_length', truncation=True)\n",
        "dummytrain_df = pd.DataFrame({'input_ids': [sample_train_datapt['input_ids'], sample_train_datapt2['input_ids']], 'attention_mask': [sample_train_datapt['attention_mask'], sample_train_datapt2['attention_mask']], 'severity': [0, 1], 'sentiment': [1, 0]})\n",
        "dummytest_df = pd.DataFrame({'input_ids': [sample_test_datapt['input_ids']], 'attention_mask': [sample_test_datapt['attention_mask']], 'severity': [0], 'sentiment': [1]})"
      ],
      "metadata": {
        "id": "pLVDBhxZN7_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummytrain_dataset = Dataset.from_pandas(dummytrain_df)\n",
        "dummytest_dataset = Dataset.from_pandas(dummytest_df)"
      ],
      "metadata": {
        "id": "HYqdmTtEPWzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "          output_dir= \"distilbert-base-cased-checkpoint\",\n",
        "          do_train=True,\n",
        "          do_eval=True,\n",
        "          num_train_epochs=1,\n",
        "          evaluation_strategy='epoch',\n",
        "          label_names=['severity', 'sentiment']\n",
        "        )"
      ],
      "metadata": {
        "id": "ja3Z_d4wQBUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "          model=model,\n",
        "          args=args,\n",
        "          train_dataset=dummytrain_dataset,\n",
        "          eval_dataset=dummytest_dataset,\n",
        "          tokenizer=tokenizer,\n",
        "        )"
      ],
      "metadata": {
        "id": "CNZKVXccQB9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "-myvtcO_QFRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}