{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "LgrTHKL2zQiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a0929f-4013-4917-9d32-49a95f26967b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.10.1 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive\n"
      ],
      "metadata": {
        "id": "x3JZtnbLYDo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f22J70R0Hcmh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CMSC723\n",
        "CROWD = \"project_materials/umd_reddit_suicidewatch_dataset_v2/crowd\"\n",
        "EXPERT = \"project_materials/umd_reddit_suicidewatch_dataset_v2/expert\""
      ],
      "metadata": {
        "id": "x1Srsq_zJbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "Q9KCs9sCYMAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets.load import load_metric\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8R0QWyz-OOlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train = pd.read_csv(join(CROWD, \"train\", \"crowd_train.csv\"))\n",
        "crowd_train_posts = pd.read_csv(join(CROWD, \"train\", \"shared_task_posts.csv\"))\n",
        "crowd_test = pd.read_csv(join(CROWD, \"test\", \"crowd_test.csv\"))\n",
        "crowd_test_posts = pd.read_csv(join(CROWD, \"test\", \"shared_task_posts_test.csv\"))"
      ],
      "metadata": {
        "id": "HpL3AGOSBS3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace NaNs"
      ],
      "metadata": {
        "id": "TWGYzMgbYSJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train.dropna(inplace=True)\n",
        "crowd_test.dropna(inplace=True)\n",
        "\n",
        "crowd_train_posts[\"post_title\"].fillna(value=\"\", inplace=True)\n",
        "crowd_test_posts[\"post_title\"].fillna(value=\"\", inplace=True)\n",
        "crowd_train_posts[\"post_body\"].fillna(value=\"\", inplace=True)\n",
        "crowd_test_posts[\"post_body\"].fillna(value=\"\", inplace=True)"
      ],
      "metadata": {
        "id": "bNyOwKBmORrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labels"
      ],
      "metadata": {
        "id": "JHiCO9xkZric"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train[\"high_severity\"] = ((((crowd_train[\"label\"] == \"c\") | (crowd_train[\"label\"] == \"d\"))).astype(\"int\"))\n",
        "crowd_test[\"high_severity\"] = ((((crowd_test[\"raw_label\"] == \"c\") | (crowd_test[\"raw_label\"] == \"d\"))).astype(\"int\"))"
      ],
      "metadata": {
        "id": "S6w5NlkxsyD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join Crowd Posts and Crowd Training Labels"
      ],
      "metadata": {
        "id": "itVmbmj4oipc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_posts[\"post_text\"] = crowd_train_posts[\"post_title\"] + \" \" + crowd_train_posts[\"post_body\"]\n",
        "crowd_test_posts[\"post_text\"] = crowd_test_posts[\"post_title\"] + \" \" + crowd_test_posts[\"post_body\"]"
      ],
      "metadata": {
        "id": "SrbOQeYnqutr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, we tried combining all of the post text for one user into a single input to feed into our LLM. However, the input size for these models is restricted, and since we do not want to truncate our input, we will instead use the suicidality assignment for each Reddit user as a weakly supervised label for the Reddit posts of that user in order to fine-tune our LLM."
      ],
      "metadata": {
        "id": "lyY-Y5igiBHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_df = pd.merge(left=crowd_train_posts, right=crowd_train, on=\"user_id\")\n",
        "crowd_test_df = pd.merge(left=crowd_test_posts, right=crowd_test, on=\"user_id\")"
      ],
      "metadata": {
        "id": "Czmee25cLDTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use DistilBERT Tokenizer on Post Text Data"
      ],
      "metadata": {
        "id": "zqYEnyJwkGoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "def tokenize_function(row):\n",
        "  return tokenizer(row[\"post_text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "crowd_train_df[\"post_text_tokenized\"] = crowd_train_df.apply(func=lambda row: tokenize_function(row), axis=1)\n",
        "crowd_test_df[\"post_text_tokenized\"] = crowd_test_df.apply(func=lambda row: tokenize_function(row), axis=1)"
      ],
      "metadata": {
        "id": "HtE3aUqmggvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_df[\"input_ids\"] = crowd_train_df.apply(func=lambda row: row[\"post_text_tokenized\"][\"input_ids\"], axis=1)\n",
        "crowd_train_df[\"attention_mask\"] = crowd_train_df.apply(func=lambda row: row[\"post_text_tokenized\"][\"attention_mask\"], axis=1)\n",
        "crowd_test_df[\"input_ids\"] = crowd_test_df.apply(func=lambda row: row[\"post_text_tokenized\"][\"input_ids\"], axis=1)\n",
        "crowd_test_df[\"attention_mask\"] = crowd_test_df.apply(func=lambda row: row[\"post_text_tokenized\"][\"attention_mask\"], axis=1)"
      ],
      "metadata": {
        "id": "zULEI4sbxSkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_df = crowd_train_df.rename(columns={\"label\": \"labels\"})\n",
        "crowd_test_df = crowd_test_df.rename(columns={\"raw_label\": \"labels\"})"
      ],
      "metadata": {
        "id": "tdiL-xgExqci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "6ipL4lQrsScF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_categories = 2\n",
        "epochs = 1\n",
        "\n",
        "print(\"Using %i categories\" % num_categories)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=num_categories)"
      ],
      "metadata": {
        "id": "_hMTID_Xru5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "          output_dir= \"distilbert-base-cased-checkpoint\",\n",
        "          do_train=True,\n",
        "          do_eval=True,\n",
        "          num_train_epochs=epochs,\n",
        "          evaluation_strategy='epoch'\n",
        "        )"
      ],
      "metadata": {
        "id": "6BsAq4RhsdVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_df.drop(labels=['post_text_tokenized'], inplace=True, axis=1)\n",
        "crowd_test_df.drop(labels=['post_text_tokenized'], inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "kLCBPCQ8yfMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_dataset = Dataset.from_pandas(crowd_train_df[:10])\n",
        "crowd_test_dataset = Dataset.from_pandas(crowd_test_df[:10])"
      ],
      "metadata": {
        "id": "Qze12euRTQXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.load import load_metric\n",
        "\n",
        "def f1_and_accuracy(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute glue_mrpc for the classification task using the\n",
        "    load_metric function.  This function is needed for the\n",
        "    compute_metrics argument of the Trainer.\n",
        "\n",
        "    You shouldn't need to modify this function.\n",
        "\n",
        "    Keyword args:\n",
        "    eval_pred -- Output from a classifier with the logits and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    metric_f1 = load_metric(\"f1\")\n",
        "    metric_accuracy = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    print({\"f1\": metric_f1.compute(predictions=predictions, references=labels)[\"f1\"], \"accuracy\": metric_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]})\n",
        "    return {\"f1\": metric_f1.compute(predictions=predictions, references=labels)[\"f1\"], \"accuracy\": metric_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]}"
      ],
      "metadata": {
        "id": "W4HszlEryZXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "          model=model,\n",
        "          args=args,\n",
        "          train_dataset=crowd_train_dataset,\n",
        "          eval_dataset=crowd_test_dataset,\n",
        "          tokenizer=tokenizer,\n",
        "          compute_metrics=f1_and_accuracy\n",
        "        )"
      ],
      "metadata": {
        "id": "1bSdthjj03KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "L0jWkZZ4PVAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict the label for each Reddit User"
      ],
      "metadata": {
        "id": "wAX-lvVqHD44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_predictions = trainer.predict(crowd_train_dataset)"
      ],
      "metadata": {
        "id": "FNib4WaRzNMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "31365b0c-be08-4b78-f156-0b67a39895d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: subreddit, timestamp, post_text, post_body, __index_level_0__, post_id, user_id, post_title. If subreddit, timestamp, post_text, post_body, __index_level_0__, post_id, user_id, post_title are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 10\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'f1': 1.0, 'accuracy': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crowd_train_df[\"suicidality_probs\"] = softmax(crowd_train_predictions.predictions, axis=1)[:,1]"
      ],
      "metadata": {
        "id": "Wg2q397pawXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_post_groups_crowd_train = crowd_train_df.groupby(\"user_id\")\n",
        "X_train = crowd_train_df.groupby(\"user_id\")['suicidality_probs'].mean().to_numpy().reshape(user_post_groups_crowd_train.ngroups, 1)\n",
        "y_train = crowd_train_df.groupby(\"user_id\")['labels'].agg(pd.Series.mode).to_numpy()"
      ],
      "metadata": {
        "id": "WD6PVoUUay6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_post_groups_crowd_test = crowd_test_df.groupby(\"user_id\")\n",
        "X_test = crowd_test_df.groupby(\"user_id\")['suicidality_probs'].mean().to_numpy().reshape(user_post_groups_crowd_test.ngroups, 1)\n",
        "y_test = crowd_test_df.groupby(\"user_id\")['labels'].agg(pd.Series.mode).to_numpy()"
      ],
      "metadata": {
        "id": "GCgZYBRoa8_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(max_depth=1)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G0jmLo5r8a4",
        "outputId": "cd496cc7-cc73-4c93-cde4-333f584097d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-fdd9441988dd>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  shortened_combined_expert_df[\"suicidality_probs\"] = probabilities[:,1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"F1:\", metrics.f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "7ovLyNhUdLco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "R2VhIUVqGf-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"saved_model\")\n",
        "filename = 'finalized_decision_tree.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "dGHu59XJGhTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}